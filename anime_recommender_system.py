# -*- coding: utf-8 -*-
"""anime-recommender-system.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TonD8dpkX2LR0ajMWUwtK8tX4Kj3bywT

## **Preparation**

- mengimpor *library* yang diperlukan
- mengunggah dataset
- mencetak dataframe serta shape dari tabel 'anime' dan
- mencetak informasi mengenai tabel 'anime'
- mencetak dataframe serta shape dari tabel 'rating'
- mencetak informasi mengenai tabel 'rating'
- mencetak ringkasan dari data 'anime'
- mencetak ringkasan dari object dataset 'anime'
- mencetak nilai 'null' dari tabel 'anime'
- menghapus nilai 'null' dari tabel anime
- mencetak ringkasan dari data 'rating'
- mencetak nilai 'null' dari tabel 'rating'
- menghapus dan mencetak entri duplikat dari tabel 'rating'
- menggabungkan dataset anime dan rating dengan menggunakan kolom anime_id sebagai acuan.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

anime = pd.read_csv("../input/anime-recommendations-database/anime.csv")
rating = pd.read_csv("../input/anime-recommendations-database/rating.csv")

print(f"Shape of The Anime Dataset : {anime.shape}")
anime.head()

print(f"Informations About Anime Dataset :\n")
print(anime.info())

print(f"Shape of The Rating Dataset : {rating.shape}")
rating.head()

print(f"Informations About Rating Dataset :\n")
print(rating.info())

print(f"Summary of The Anime Dataset :")
anime.describe()

anime.describe(include=object)

print("Null Values of Anime Dataset :")
anime.isna().sum()

print("After Dropping, Null Values of Anime Dataset :")
anime.dropna(axis = 0, inplace = True)
anime.isna().sum()

dup_anime = anime[anime.duplicated()].shape[0]
print(f"There are {dup_anime} duplicate entries among {anime.shape[0]} entries in anime dataset.")

print(f"Summary of The Rating Dataset :")
rating.describe()

print("Null Values of Rating Dataset :")
rating.isna().sum()

dup_rating = rating[rating.duplicated()].shape[0]
print(f"There are {dup_rating} duplicate entries among {rating.shape[0]} entries in rating dataset.")

rating.drop_duplicates(keep='first',inplace=True)
print(f"\nAfter removing duplicate entries there are {rating.shape[0]} entries in this dataset.")

fulldata = pd.merge(anime,rating,on="anime_id",suffixes=[None, "_user"])
fulldata = fulldata.rename(columns={"rating_user": "user_rating"})

print(f"Shape of The Merged Dataset : {fulldata.shape}")
print(f"\nGlimpse of The Merged Dataset :")

fulldata.head(10)

"""# **EDA**

- ekplorisasi dan visualisasi terhadap komunitas masing-masing anime
- ekplorisasi dan visualisasi terhadap kategori anime
- ekplorisasi dan visualisasi terhadap rating tertinggi dari 10 anime
- ekplorisasi dan visualisasi terhadap genre anime
"""

top_anime = fulldata.copy()
top_anime.drop_duplicates(subset ="name", keep = "first", inplace = True)
top_anime_temp1 = top_anime.sort_values(["members"],ascending=False)

plt.subplots(figsize=(20,8))
p = sns.barplot(x=top_anime_temp1["name"][:10],
                y=top_anime_temp1["members"], 
                saturation = 1, 
                linewidth = 1)

p.axes.set_title("\nTop Anime Community\n", fontsize=25)
plt.ylabel("Total Member" , fontsize = 20)
plt.xlabel("\nAnime Title" , fontsize = 20)
# plt.yscale("log")
plt.xticks(rotation = 80)
for container in p.containers:
    p.bar_label(container,label_type = "center",padding = 6,size = 15,color = "black",rotation = 90)

sns.despine(left=True, bottom=True)
plt.show()

plt.subplots(figsize = (20,8))
p = sns.countplot(x = top_anime_temp1["type"], 
                  order = top_anime_temp1["type"].value_counts().index, saturation = 1,linewidth = 3)

p.axes.set_title("\nAnime Categories Hub\n" ,fontsize = 25)
plt.ylabel("Total Anime" ,fontsize = 20)
plt.xlabel("\nAnime Category" ,fontsize = 20)
plt.xticks(rotation = 0)
for container in p.containers:
    p.bar_label(container,label_type = "center",padding = 10,size = 25,color = "black",rotation = 0)

sns.despine(left = True, bottom = True)
plt.show()

top_anime_temp2 = top_anime.sort_values(["rating"],ascending=False)

plt.subplots(figsize=(20,8))
p = sns.barplot(x=top_anime_temp2["name"][:10],y=top_anime_temp2["rating"], saturation=1, linewidth = 2)
p.axes.set_title("\nTop Animes Based On Ratings\n",fontsize = 25)
plt.ylabel("Average Rating",fontsize = 20)
plt.xlabel("\nAnime Title",fontsize = 20)

plt.xticks(rotation = 80)
for container in p.containers:
    p.bar_label(container,label_type = "center",padding = 10,size = 15,color = "black",rotation = 0)

sns.despine(left=True, bottom=True)
plt.show()

top_anime_temp3 = top_anime[["genre"]]
top_anime_temp3["genre"] = top_anime_temp3["genre"].str.split(", | , | ,")
top_anime_temp3 = top_anime_temp3.explode("genre")
top_anime_temp3["genre"] = top_anime_temp3["genre"].str.title()
print(f'Total unique genres are {len(top_anime_temp3["genre"].unique())}')
top_anime_temp3["genre"].value_counts().to_frame().head(10)

df = top_anime_temp3["genre"].value_counts().to_frame().head(10)
plt.subplots(figsize=(20,8))
p = sns.barplot(x=df.index, y=df["genre"],saturation=1, linewidth = 2)
p.axes.set_title("\nTotal Anime Based On Genre\n",fontsize = 25)
plt.ylabel("",fontsize = 20)
plt.xlabel("\n Anime Genre",fontsize = 20)

plt.xticks(rotation = 80)
for container in p.containers:
    p.bar_label(container,label_type = "center",padding = 10,size = 15,color = "black",rotation = 0)

sns.despine(left=True, bottom=True)
plt.show()

data = fulldata.copy()
data["user_rating"].replace(to_replace = -1 , value = np.nan ,inplace=True)
data = data.dropna(axis = 0)
print("Null values after final pre-processing :")
data.isna().sum()

"""There are a lot of users who have rated only once, even if they have rated 5 animes, it can't be considered as a valuable record for recommendation. So we will consider minimum 50 ratings by the user as a threshold value."""

selected_users = data["user_id"].value_counts()
data = data[data["user_id"].isin(selected_users[selected_users >= 50].index)]

"""We will create a pivot table consists of rows as title and columns as user id, this will help us to create sparse matrix which can be very helpful in finding the cosine similarity.

## **Data Cleaning**

- membersihkan teks dari beberapa karakter tidak diinginkan di dalam kolom 'name' dari dataset.
- beberapa karakter yang dihilangkan antara lain ", .hack//, ', A's, I', dan &.
- memperbarui kolom 'name' dengan data yang sudah dibersihkan
"""

import re
def text_cleaning(text):
    text = re.sub(r'&quot;', '', text)
    text = re.sub(r'.hack//', '', text)
    text = re.sub(r'&#039;', '', text)
    text = re.sub(r'A&#039;s', '', text)
    text = re.sub(r'I&#039;', 'I\'', text)
    text = re.sub(r'&amp;', 'and', text)
    
    return text

data["name"] = data["name"].apply(text_cleaning)

df = pd.DataFrame(data)

"""## **Data Training**
- mengimpor *library* yang diperlukn
- melakukan encoding pada kolom 'anime_id' dan 'user_id'
- normalisasi dan pembagian data untuk latih dan validasi pada dataframe yang berisi data *rating anime*.
- deklarasi dari model yang digunakan dalam sistem rekomendasi ini
- melakukan proses *compile* dari model 
- memulai proses pelatihan model
"""

import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

user_ids = df["user_id"].unique().tolist()
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
userencoded2user = {i: x for i, x in enumerate(user_ids)}
anime_ids = df["anime_id"].unique().tolist()
anime2anime_encoded = {x: i for i, x in enumerate(anime_ids)}
anime_encoded2anime = {i: x for i, x in enumerate(anime_ids)}
df["user"] = df["user_id"].map(user2user_encoded)
df["anime"] = df["anime_id"].map(anime2anime_encoded)

num_users = len(user2user_encoded)
num_anime = len(anime_encoded2anime)
df["user_rating"] = df["user_rating"].values.astype(np.float32)
# min and max ratings will be used to normalize the ratings later
min_rating = min(df["user_rating"])
max_rating = max(df["user_rating"])

print(
    "Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}".format(
        num_users, num_anime, min_rating, max_rating
    )
)

df = df.sample(frac=1, random_state=42)
x = df[["user", "anime"]].values
# Normalize the targets between 0 and 1. Makes it easy to train.
y = df["user_rating"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
# Assuming training on 90% of the data and validating on 10%.
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)

EMBEDDING_SIZE = 128

class RecommenderNet(keras.Model):
    def __init__(self, num_users, num_movies, embedding_size, **kwargs):
        super().__init__(**kwargs)
        self.num_users = num_users
        self.num_movies = num_movies
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.movie_embedding = layers.Embedding(
            num_movies,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.movie_bias = layers.Embedding(num_movies, 1)
        self.dense_1 = layers.Dense(64, activation='relu')
        self.dense_2 = layers.Dense(32, activation='relu')
        self.dense_3 = layers.Dense(1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])
        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)
        # Add all the components (including bias)
        x = dot_user_movie + user_bias + movie_bias
        x = self.dense_1(x)
        x = self.dense_2(x)
        x = self.dense_3(x)
        # The sigmoid activation forces the rating to between 0 and 1
        return tf.nn.sigmoid(x)
    
model = RecommenderNet(num_users, num_anime, 50) # inisialisasi model
model.build(input_shape=(None, 2))
model.summary()

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()])

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 512,
    epochs = 20,
    validation_data = (x_val, y_val))

"""## **Evaluation**
- membuat visualisasi dari proses training dan validasi
- melakukan testing pada model yang sudah selesai dilatih
"""

import matplotlib.pyplot as plt

# Extract the training and validation loss and rmse
loss = history.history['loss']
val_loss = history.history['val_loss']
rmse = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']

# Create two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Plot the loss on the first subplot
ax1.plot(loss, label='Training Loss')
ax1.plot(val_loss, label='Validation Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend()

# Plot the rmse on the second subplot
ax2.plot(rmse, label='Training RMSE')
ax2.plot(val_rmse, label='Validation RMSE')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('RMSE')
ax2.legend()

plt.show()

anime_df = pd.read_csv("/kaggle/input/anime-recommendations-database/anime.csv")

# Let us get a user and see the top recommendations.
user_id = df.user_id.sample(1).iloc[0]
anime_watched_by_user = df[df.user_id == user_id]
anime_not_watched = anime_df[
    ~anime_df["anime_id"].isin(anime_watched_by_user.anime_id.values)
]["anime_id"]
anime_not_watched = list(
    set(anime_not_watched).intersection(set(anime2anime_encoded.keys()))
)
anime_not_watched = [[anime2anime_encoded.get(x)] for x in anime_not_watched]
user_encoder = user2user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)
)
ratings = model.predict(user_anime_array).flatten()
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded2anime.get(anime_not_watched[x][0]) for x in top_ratings_indices
]

print("Showing recommendations for user: {}".format(user_id))
print("====" * 9)
print("Anime with high ratings from user")
print("----" * 8)
top_anime_user = (
    anime_watched_by_user.sort_values(by="rating", ascending=False)
    .head(5)
    .anime_id.values
)
anime_df_rows = anime_df[anime_df["anime_id"].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.name, ":", row.genre)

print("----" * 8)
print("Top 10 anime recommendations")
print("----" * 8)
recommended_anime = anime_df[anime_df["anime_id"].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.name, ":", row.genre)